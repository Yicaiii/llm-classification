
# LLM Preference Classification (Kaggle)

This project implements an end-to-end pipeline for **human preference prediction between large language model (LLM) responses**, based on pairwise comparison data from the Kaggle competition **“LLM Classification: Fine-Tuning”**.

The goal is to train a supervised preference model that predicts which response is preferred by humans — **Response A**, **Response B**, or **Tie** — given a prompt and two candidate responses.

---

## 1. Project Overview

Large language models are increasingly evaluated not only by accuracy, but by **human preference**.  
This project formulates preference learning as a **multi-class classification problem**, serving as a simplified but realistic version of the **reward / preference modeling stage** commonly used in RLHF (Reinforcement Learning from Human Feedback).

The project focuses on:
- Clean and reproducible preprocessing of real-world LLM comparison data  
- A modular, notebook-based machine learning pipeline  
- Clear separation between training and inference  
- Practical engineering trade-offs under limited computational resources  

---

## 2. Dataset Description

The dataset is provided by the Kaggle competition and is derived from **Chatbot Arena** comparisons.

Each sample contains:
- A user **prompt**
- Two responses (**Response A** and **Response B**) generated by different LLMs
- A human judgment indicating whether Response A wins, Response B wins, or the result is a tie

Exploratory analysis shows that the dataset is **imbalanced**, with tie cases occurring much less frequently than clear preferences for one response.

---

## 3. Problem Formulation

The task is formulated as a **3-class classification problem**:

- `0`: Response A is preferred  
- `1`: Response B is preferred  
- `2`: Tie  

Each training example is represented as a single text sequence constructed by concatenating:

```

Prompt + Response A + Response B

```

This allows the model to jointly reason over the question and both candidate answers.

---

## 4. Methodology

### 4.1 Data Preprocessing

The preprocessing pipeline performs the following steps:

- Merge the three binary preference indicators into a single multi-class label
- Normalize prompts that are stored as lists into a single string
- Construct model inputs by concatenating prompt and responses
- Apply Unicode cleaning to remove invalid surrogate characters commonly found in LLM-generated text
- Save the processed dataset to disk to ensure a clean interface between preprocessing and training

---

### 4.2 Model and Training

A **BERT-based sequence classification model** (`bert-base-uncased`) is used to predict human preference labels.

Key aspects of the training setup:
- Cross-entropy loss for 3-class classification
- Stratified train–validation split to preserve class distribution
- Fine-tuning implemented using HuggingFace’s `Trainer`

Due to computational constraints, full training is **not executed locally on CPU**.  
Instead, a small subset of the data is used as a **sanity check** to validate the correctness of the training pipeline, while the full training code is kept unchanged for reproducibility.

---

### 4.3 Inference and Submission

The inference pipeline:
- Applies the same preprocessing logic used during training
- Tokenizes test samples consistently with the training setup
- Converts model logits into probabilities using softmax
- Generates a Kaggle-compatible `submission.csv` file containing:
  - Probability that Response A wins
  - Probability that Response B wins
  - Probability of a tie

---

## 5. Project Structure

The repository is organized as a clear, modular pipeline:

```

llm-preference-classification/
│
├── data/
│   ├── raw/                # Original Kaggle data
│   └── processed/          # Preprocessed training data
│
├── notebooks/
│   ├── 01_eda.ipynb        # Exploratory data analysis
│   ├── 02_preprocessing.ipynb
│   │                       # Text normalization and label construction
│   ├── 03_model_training.ipynb
│   │                       # BERT fine-tuning (sanity check on CPU)
│   └── 04_inference_submission.ipynb
│                           # Inference and Kaggle submission
│
├── outputs/
│   └── predictions/
│       └── submission.csv
│
└── README.md

```

The notebooks follow the logical workflow:
**EDA → preprocessing → training → inference**, ensuring clarity and reproducibility.

---

## 6. Training Notes (CPU Limitation)

Full fine-tuning of BERT with long input sequences (512 tokens) is computationally expensive on CPU.

In this project:
- The full training pipeline is implemented for completeness and reproducibility
- Local execution is limited to a small subset of the data for sanity checking
- In practice, full training is expected to be performed on GPU or cloud infrastructure

---

## 7. Key Design Choices

- Modeling preference learning as a **3-class classification task**
- Concatenating prompt and responses for joint semantic modeling
- Using stratified splitting to handle class imbalance
- Separating preprocessing, training, and inference into modular notebooks
- Prioritizing pipeline correctness and reproducibility over local training speed

---

## 8. Limitations and Future Work

- Full-scale training was limited by CPU resources
- More efficient models (e.g., DistilBERT) could be explored
- Pairwise ranking or reward modeling approaches could better align with RLHF pipelines
- Additional bias analysis on preference predictions could be conducted

---

## 9. Conclusion

This project demonstrates a complete and reproducible pipeline for **LLM preference classification**, bridging practical machine learning engineering with concepts used in modern LLM alignment and evaluation.

While designed under limited computational resources, the structure and methodology directly extend to real-world GPU-based training setups.
```
ss
---
git